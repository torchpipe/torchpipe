{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"index.html","level":1,"title":"Home","text":"Concurrent Requests<sup>1</sup> torch2trt TorchPipe TorchPipe w/ Thrift Triton Inference Server Triton Ensemble w/ DALI 1 90 124 92 20 66 2 - 159 156 45 114 5 - 267 265 89 233 10 - 315 304 161 307 Line of Code very low low low middle high <p>TorchPipe is a modular backend framework for DNN serving, designed on top of standardized computation and scheduling layers. Its primary goal is to deliver high performance with minimal user configuration.</p> <p>If you find an issue, please let us know!</p> <ol> <li> <p>Test environment. ↩</p> </li> </ol>","path":["Home"],"tags":[]},{"location":"installation.html","level":1,"title":"Installation","text":"","path":["Installation"],"tags":[]},{"location":"installation.html#build-from-source","level":2,"title":"build from source","text":"","path":["Installation"],"tags":[]},{"location":"installation.html#inside-ngc-docker-containers","level":3,"title":"Inside NGC Docker Containers","text":"","path":["Installation"],"tags":[]},{"location":"installation.html#test-on-2505-2405-2305-and-2212","level":4,"title":"test on 25.05, 24.05, 23.05, and 22.12","text":"<pre><code>git clone https://github.com/torchpipe/torchpipe.git\ncd torchpipe/\n\nimg_name=nvcr.io/nvidia/pytorch:25.05-py3 # you can also try 24.05, 23.05, 22.12, but may need to upgrade pip: python -m pip install --upgrade pip\n\ndocker run --rm --gpus all -it --rm --network host \\\n    -v $(pwd):/workspace/ --ipc=host --ulimit memlock=-1 --ulimit stack=67108864\\\n    -w /workspace/ \\\n    $img_name \\\n    bash\n\n# pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n# python -m pip install --upgrade pip # for 23.05, 22.12, 24.05\ncd /workspace &amp;&amp; pip install . -v &amp;&amp; cd /workspace/plugins/torchpipe &amp;&amp; pip install . --no-build-isolation\n\n\n# JIT compile built-in backends\npython -c \"import torchpipe\"\n</code></pre>","path":["Installation"],"tags":[]},{"location":"installation.html#rebuild-the-core-library-omniback-no-isolation","level":3,"title":"Rebuild the core library Omniback: No isolation","text":"<p>Omniback is usually not needed to be rebuilt.</p> <p>However, if you want to modify the core library or encounter any compatibility issues, you can rebuild Omniback first.</p> <pre><code>git clone https://github.com/torchpipe/torchpipe.git --recursive\ncd torchpipe/\n\npython -m pip install --upgrade pip \n\npip install --upgrade scikit_build_core fire ninja setuptools-scm setuptools apache-tvm-ffi \n\npip install . --no-deps --no-build-isolation -v\n\ncd plugins/torchpipe\n\npip install . --no-deps --no-build-isolation -v \n\npython -c \"import torchpipe\"\n</code></pre>","path":["Installation"],"tags":[]},{"location":"installation.html#_1","level":2,"title":"基础环境镜像","text":"<p>可如下构建基础环境镜像： <pre><code># 显卡驱动 &gt;=545\n# cuda12.6 + trt10.14\ndocker build -t torchpipe:base_trt1014 -f docker/DockerfileCuda12_TRT1014 .\n</code></pre></p>","path":["Installation"],"tags":[]},{"location":"installation.html#dependency-compatibility","level":3,"title":"Dependency Compatibility","text":"Library Required Version Recommended Version Notes TensorRT [<code>8.5</code>, <code>~10.9</code>] <code>9.3</code>, <code>10.9</code> Not all version tested OpenCV <code>&gt;=4</code> <code>~=4.5.0</code> PyTorch <code>&gt;=1.10.2</code> <code>~=2.7.0</code> CUDA [<code>11</code>,<code>12</code>]","path":["Installation"],"tags":[]},{"location":"usage/basic_usage.html","level":1,"title":"Basic usage","text":"","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#thread-safe-local-inference","level":2,"title":"Thread-Safe Local Inference","text":"<p>For convenience, let's assume that the TensorRT inference functionality is encapsulated as a \"computational backend\" named <code>TensorrtTensor</code>. Since the computation occurs on the GPU device, we add <code>SyncTensor</code> to represent the stream synchronization operation on the GPU.</p> Configuration Parameter Description backend \"SyncTensor[TensorrtTensor]\" The computational backend, like TensorRT inference itself, is not thread-safe. max 4 The maximum batch size supported by the model, used for model conversion (ONNX-&gt;TensorRT). <p>By default, TorchPipe wraps an extensible single-node scheduling backend on top of this \"computational backend,\" which provides the following three basic capabilities:</p> <ul> <li>Thread safety of the forward interface</li> <li> <p>Multi-instance parallelism</p> Configuration Default Description instance_num 1 Perform inference tasks in parallel with multiple model instances. </li> <li> <p>Batching</p> Configuration Default Description batching_timeout 0 The timeout in milliseconds. </li> </ul>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#performance-tuning-tips","level":3,"title":"Performance tuning tips","text":"<p>Summarizing the above steps, we obtain the necessary parameters for inference of ResNet18 under TorchPipe:</p> <pre><code>import torchpipe as tp\nimport torch\n\nconfig = {\n    # Single-node scheduler parameters: \n    \"instance_num\": 2,\n    \"batching_timeout\": 5,\n    # Computational backend:\n    \"backend\": \"SyncTensor[TensorrtTensor]\",\n    # Computational backend parameters:\n    \"model\": \"resnet18_-1x3x224x224.onnx\",\n    \"max\": 4\n}\n\n# Initialization\nmodels = tp.pipe(config)\ndata = torch.ones(1, 3, 224, 224).cuda()\n\n## Forward\ninput = {\"data\": data}\nmodels(input) # &lt;== Can be called from multiple threads\nresult: torch.Tensor = torch.from_dlpack(input[\"result\"]) # \"result\" does not exist if the inference failed\n</code></pre> <p>Assuming that we want to support a maximum of 10 clients/concurrent requests, the <code>instance_num</code> is usually set to 2, so that we can handle up to <code>instance_num * max = 8</code> requests at most.</p>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#sequential","level":2,"title":"Sequential","text":"<p><code>Sequential</code> can link multiple backends together. In other words, <code>Sequential[DecodeTensor,ResizeTensor,CvtColorTensor,SyncTensor]</code> and <code>Sequential[DecodeMat,ResizeMat]</code> are valid backends.</p> <p>During the forward execution of <code>Sequential[DecodeMat,ResizeMat]</code>, the data (dict) will go through the following process in sequence:</p> <ul> <li>Execute <code>DecodeMat</code>: <code>DecodeMat</code> reads <code>data</code> and assigns the result to <code>result</code> and <code>color</code>.</li> <li>Conditional control flow: attempts to assign the value of <code>result</code> in the data to <code>data</code> and deletes <code>result</code>.</li> <li>Execute <code>ResizeMat</code>: <code>ResizeMat</code> reads <code>data</code> and assigns the result to the <code>result</code> key.</li> </ul> <p><code>Sequential</code> can be abbreviated as <code>S</code>. </p>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#custom-backends","level":2,"title":"Custom backends","text":"<p>A major problem in business is that the preset backends (computational backend/scheduling backend/RPC backend/cross-process backend, etc.) cannot cover all requirements.   <code>Torchpipe</code> treat the backend itself is also an API oriented towards users. </p>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#basic-types","level":3,"title":"Basic Types","text":"","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#any","level":4,"title":"any","text":"<p>Similar to <code>std::any</code> in C++17, we have defined a type-erased container, <code>om::any</code>, with an almost identical interface.</p>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#dict","level":4,"title":"dict","text":"<p>As a data carrier, similar to Python's <code>dict</code>, we have also defined the following <code>dict</code> in C++: <pre><code>#ifndef CUSTOM_DICT\nusing dict = std::shared_ptr&lt;std::unordered_map&lt;std::string, om::any&gt;&gt;;\n#else\n#endif\n</code></pre></p>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/basic_usage.html#backend","level":3,"title":"Backend","text":"<p>Torchpipe limits the basic elements of the backend to:</p> <ul> <li>Initialization: parameter configuration</li> <li>Forward: input/output interface</li> <li>max/min: batch range of data</li> </ul>","path":["Usage","Basic usage"],"tags":[]},{"location":"usage/links.html","level":1,"title":"Links between multiple nodes","text":"","path":["Usage","Links between multiple nodes"],"tags":[]},{"location":"usage/links.html#serial-nodes","level":2,"title":"Serial Nodes","text":"<p>The <code>Sequential[DecodeMat,ResizeMat]</code> backend can be represented in a multi-node manner as follows: <pre><code># irrelevant parameters are ignored \n[decode]\n# highlight-next-line\nnext=\"resize\"\n\n[resize]\n</code></pre></p> <p>The <code>decode-&gt;resize</code> forms a directed acyclic graph consisting of two nodes, where <code>decode</code> is the root node and <code>resize</code> is the child node.</p>","path":["Usage","Links between multiple nodes"],"tags":[]},{"location":"usage/links.html#multiple-branches","level":2,"title":"Multiple Branches","text":"<p>If a node has multiple subsequent nodes, they can be separated by commas in the <code>next</code> field.</p> <p>Consider the following multi-node system:</p> <pre><code>flowchart TD\n    A --&gt; B\n    A --&gt; C\n    C --&gt; D[output]\n    B --&gt; D\n</code></pre> <p>The node relationship can be represented as:</p> <pre><code># irrelevant parameters are ignored \n[A]\n# highlight-next-line\nnext=\"B,C\"\n[B]\nnext=\"output\"\n[C]\nnext=\"output\"\n[output]\n</code></pre>","path":["Usage","Links between multiple nodes"],"tags":[]},{"location":"usage/links.html#data-flow","level":2,"title":"Data Flow","text":"<p>The original input <code>dict</code> data is processed by node <code>A</code>, and both nodes <code>B</code> and <code>C</code> request data from node <code>A</code>. To avoid accessing modified data, both nodes <code>B</code> and <code>C</code> make a copy of the data from node <code>A</code>.</p>","path":["Usage","Links between multiple nodes"],"tags":[]},{"location":"usage/links.html#map","level":2,"title":"map: Collecting Data from Preceding Nodes","text":"<p><pre><code># irrelevant parameters are ignored \n[output]\n# highlight-next-line\nmap=\"B[result:B_result],C[result:data]\"\n</code></pre> The <code>output</code> configuration uses <code>map</code> to map the <code>result</code> of node B to its own <code>B_result</code>, and the <code>result</code> of node C to <code>data</code>. </p> <p><code>map</code> can also map multiple key-value pairs from preceding nodes to itself, such as <code>map=\"B[1:0,1:1,2:data]\"</code>.</p> <p>If there is no ambiguity, the node name can be omitted in <code>map</code>, such as <code>map=\"[result:data]\"</code>.</p> <p>Note:</p> <ul> <li>The combined data must include <code>data</code>.</li> <li>Like <code>next</code>, <code>map</code> also creates a reference to the source node. If a node is referenced by two or more subsequent nodes, its data will be copied when requested by the subsequent nodes.</li> <li>When requesting data through <code>map</code>, the source data is always copied.</li> </ul>","path":["Usage","Links between multiple nodes"],"tags":[]}]}