#include "OpenCVResizeTensor.hpp"
#include <cstddef>
#include <torch/torch.h>
// #include <cuda.h>
#include <cuda_runtime.h>
#include <nppcore.h>
#include <fstream>
#include <iostream>
#include <numeric>
#include <thread>
#include "base_logging.hpp"
#include "c10/cuda/CUDAStream.h"
#include "hw_batching.hpp"
#include "nppi_geometry_transforms.h"
#include "reflect.h"
#include "torch_utils.hpp"
#include "ipipe_utils.hpp"
#include "ppl/cv/cuda/resize.h"
#include "exception.hpp"

namespace ipipe {
namespace __tmp {
torch::Tensor nppiresize_topleft(torch::Tensor& input, int target_h, int target_w, int pad_value) {
  if (input.scalar_type() != torch::kByte && input.scalar_type() != torch::kFloat) {
    throw std::invalid_argument("error type: need scalar type: kByte or kFloat");
  }

  int img_h = input.sizes()[0];
  int img_w = input.sizes()[1];
  int c = input.sizes()[2];

  auto single_scale = std::max(float(img_w) / float(target_w), float(img_h) / float(target_h));
  int true_w = floor(img_w / single_scale);
  int true_h = img_h / single_scale;
  auto options = torch::TensorOptions()
                     .device(torch::kCUDA, -1)
                     .dtype(input.scalar_type())  // torch::kByte
                     .layout(torch::kStrided)
                     .requires_grad(false);
  auto output_tensor = torch::full({target_h, target_w, c}, int(pad_value), options);
  if (!output_tensor.is_contiguous()) {
    output_tensor = output_tensor.contiguous();
  }

  auto* output = output_tensor.data_ptr();

  if (!input.is_contiguous()) input = input.contiguous();
  void* image = input.data_ptr();

  // typedef enum {
  //     NPPI_INTER_UNDEFINED = 0,
  //     NPPI_INTER_NN = 1,          /**<  最近邻插值 */
  //     NPPI_INTER_LINEAR = 2,      /**<  线性插值 */
  //     NPPI_INTER_CUBIC = 4,       /**<  三次插值 */
  //     NPPI_INTER_CUBIC2P_BSPLINE, /**<  Two-parameter cubic filter (B=1,
  //     C=0)
  //                                  */
  //     NPPI_INTER_CUBIC2P_CATMULLROM, /**<  Two-parameter cubic filter (B=0,
  //                                       C=1/2) */
  //     NPPI_INTER_CUBIC2P_B05C03,     /**<  Two-parameter cubic filter
  //     (B=1/2,
  //                                       C=3/10) */
  //     NPPI_INTER_SUPER = 8,          /**<  Super sampling. */
  //     NPPI_INTER_LANCZOS = 16,       /**<  Lanczos filtering. */
  //     NPPI_INTER_LANCZOS3_ADVANCED =
  //         17, /**<  Generic Lanczos filtering with order 3. */
  //     NPPI_SMOOTH_EDGE = (1 << 31) /**<  Smooth edge filtering. */
  // } NppiInterpolationMode;

  NppiInterpolationMode eInterploationMode = NPPI_INTER_CUBIC;
  if (target_w < img_w && target_h < img_h) {
    eInterploationMode = NPPI_INTER_SUPER;
    // https://forums.developer.nvidia.com/t/npp-library-functions-nppiresize-8u-c3r-and-nppibgrtolab-8u-c3r-differ-from-cv-resize-output/66608/7
  }

  NppiSize image_a_size = {.width = img_w, .height = img_h};
  NppiRect image_a_roi = {.x = 0, .y = 0, .width = img_w, .height = img_h};

  NppiSize image_b_size = {.width = target_w, .height = target_h};
  NppiRect image_b_roi = {.x = 0, .y = 0, .width = true_w, .height = true_h};

  NppStatus result;
  if (input.scalar_type() != torch::kByte) {
    result = nppiResize_8u_C3R((Npp8u*)image, img_w * 3, image_a_size, image_a_roi, (Npp8u*)output,
                               target_w * 3, image_b_size, image_b_roi, eInterploationMode);
  } else {
    result = nppiResize_32f_C3R((Npp32f*)image, img_w * 3 * sizeof(Npp32f), image_a_size,
                                image_a_roi, (Npp32f*)output, target_w * 3 * sizeof(Npp32f),
                                image_b_size, image_b_roi, eInterploationMode);
  }

  if (result != NPP_SUCCESS) {
    std::cout << "nppiresize_topleft Error executing Resize -- code: " << result << " " << target_w
              << " " << img_w << " " << target_h << " " << img_h << std::endl;
    return torch::Tensor();
  }

  return output_tensor;
}

torch::Tensor nppiresize(torch::Tensor input, int target_h, int target_w) {
  int num = input.sizes().size() == 4 ? input.sizes()[0] : 1;
  if (num != 1 || input.size(-1) != 3) {
    throw std::invalid_argument("nppiresize ;: error num or input.size" +
                                std::to_string(input.size(0)) + std::to_string(input.size(1)) +
                                std::to_string(input.size(2)) + std::to_string(input.size(-1)));
  }
  if (input.scalar_type() != torch::kByte && input.scalar_type() != torch::kFloat) {
    throw std::invalid_argument("error datatype of tensor.  need datatype float or char.");
  }

  int img_h = input.sizes()[0];
  int img_w = input.sizes()[1];
  int c = input.sizes()[2];

  auto options = torch::TensorOptions()
                     .device(torch::kCUDA, -1)
                     .dtype(input.scalar_type())  // torch::kByte
                     .layout(torch::kStrided)
                     .requires_grad(false);

  auto output_tensor = torch::full({target_h, target_w, c}, 0, options);
  auto* output = output_tensor.data_ptr();

  if (!input.is_contiguous()) input = input.contiguous();
  void* image = input.data_ptr();

  NppiInterpolationMode eInterploationMode = NPPI_INTER_CUBIC;
  if (target_w < img_w && target_h < img_h) {
    eInterploationMode = NPPI_INTER_SUPER;
    // https://forums.developer.nvidia.com/t/npp-library-functions-nppiresize-8u-c3r-and-nppibgrtolab-8u-c3r-differ-from-cv-resize-output/66608/7
  }

  // eInterploationMode = NPPI_INTER_LINEAR;
  NppiSize image_a_size = {.width = img_w, .height = img_h};
  NppiRect image_a_roi = {.x = 0, .y = 0, .width = img_w, .height = img_h};

  NppiSize image_b_size = {.width = target_w, .height = target_h};
  NppiRect image_b_roi = {.x = 0, .y = 0, .width = target_w, .height = target_h};
  NppStatus result;
  if (input.scalar_type() == torch::kByte) {
    result = nppiResize_8u_C3R((Npp8u*)image, img_w * 3, image_a_size, image_a_roi, (Npp8u*)output,
                               target_w * 3, image_b_size, image_b_roi, eInterploationMode);

  } else {
    result = nppiResize_32f_C3R((Npp32f*)image, img_w * 3 * sizeof(Npp32f), image_a_size,
                                image_a_roi, (Npp32f*)output, target_w * 3 * sizeof(Npp32f),
                                image_b_size, image_b_roi, eInterploationMode);
  }
  if (result != NPP_SUCCESS) {
    SPDLOG_ERROR(
        "nppiresize: Error executing Resize -- code: {} img_w {} img_h {} target_w {} target_h {}",
        result, img_w, img_h, target_w, target_h);
    throw std::runtime_error("nppiresize falied");
  }

  return output_tensor;
}
}  // namespace __tmp
bool OpenCVResizeTensor::init(const std::unordered_map<std::string, std::string>& config_param,
                              dict dict_config) {
  params_ = std::unique_ptr<Params>(new Params({}, {"resize_h", "resize_w"}, {}, {}));
  if (!params_->init(config_param)) return false;

  TRACE_EXCEPTION(resize_h_ = std::stoi(params_->operator[]("resize_h")));
  TRACE_EXCEPTION(resize_w_ = std::stoi(params_->operator[]("resize_w")));
  if (resize_h_ > 1024 * 1024 || resize_w_ > 1024 * 1024 || resize_h_ < 1 || resize_w_ < 1 ||
      resize_w_ * resize_h_ > 1024 * 1024 * 100) {
    SPDLOG_ERROR("OpenCVResizeTensor: illigle h or w: h=" + std::to_string(resize_h_) +
                 "w=" + std::to_string(resize_w_));
    return false;
  }

  return true;
}

void OpenCVResizeTensor::forward(dict input_dict) {
  auto input_tensor = dict_get<torch::Tensor>(input_dict, TASK_DATA_KEY);

  input_tensor = img_1chw_guard(input_tensor);  //.to(torch::kFloat);

  torch::Tensor im_resize;

  if (input_tensor.size(2) == resize_h_ && input_tensor.size(3) == resize_w_) {
    im_resize = input_tensor;
  } else {
    input_tensor = img_hwc_guard(input_tensor);
    if (input_tensor.is_cpu()) {
      input_tensor = input_tensor.cuda();
    }
    if (!input_tensor.is_contiguous()) input_tensor = input_tensor.contiguous();
    auto options = torch::TensorOptions()
                       .device(torch::kCUDA, -1)
                       .dtype(input_tensor.dtype())  // scalar_type dtype torch::kByte
                       .layout(torch::kStrided)
                       .requires_grad(false);
    im_resize = torch::empty({resize_h_, resize_w_, input_tensor.size(2)},  //, max.d[2], max.d[3]
                          options, torch::MemoryFormat::Contiguous);
    const int channel = input_tensor.size(2);
    // IPIPE_ASSERT(channel == 3);

    auto code = cudaGetLastError();
    if (code != cudaSuccess) {
      SPDLOG_ERROR(std::string("C0UDA error: ") + cudaGetErrorString(code));
      return;
    }

    if (input_tensor.scalar_type() == torch::ScalarType::Byte) {
      assert(resize_h_ > 0 && resize_w_ > 0);

      auto ret = ppl::cv::cuda::resize(
          input_tensor.data_ptr<unsigned char>(), (int)input_tensor.size(0),
          (int)input_tensor.size(1), channel, int(input_tensor.size(1) * channel),
          im_resize.data_ptr<unsigned char>(), (int)resize_h_, (int)resize_w_,
          int(resize_w_ * channel), ppl::cv::InterpolationType::INTERPOLATION_LINEAR,
          c10::cuda::getCurrentCUDAStream());
      if (ret != ppl::common::RC_SUCCESS) {
        throw std::runtime_error("Error in ppl::cv::cuda::resize. ret = " + std::to_string(ret));
      }
    } else if (input_tensor.scalar_type() == torch::ScalarType::Float) {
      auto ret = ppl::cv::cuda::resize(
          input_tensor.data_ptr<float>(), input_tensor.size(0), input_tensor.size(1), channel,
          input_tensor.size(1) * channel, im_resize.data_ptr<float>(), resize_h_, resize_w_,
          resize_w_ * channel, ppl::cv::InterpolationType::INTERPOLATION_LINEAR,
          c10::cuda::getCurrentCUDAStream());
      if (ret != ppl::common::RC_SUCCESS) {
        throw std::runtime_error("Error in ppl::cv::cuda::resize. ret = " +
                                 std::to_string(int(ret)));
      }
    } else {
      throw std::runtime_error("unsupported input_tensor.scalar_type");
    }
    im_resize.permute({2, 0, 1}).unsqueeze(0);
  }

  (*input_dict)[TASK_RESULT_KEY] = im_resize;
}

IPIPE_REGISTER(Backend, OpenCVResizeTensor, "OpenCVResizeTensor");

}  // namespace ipipe