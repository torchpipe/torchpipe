from modules import ClipModule, UNetModule, VaeModule, SafetyModule
import modules
import torch
import time
import json
import os
import fire
from copy import deepcopy
from tqdm import tqdm
from datetime import datetime
# from stable_diffusion_v1_5.stable_diffusion_pipeline import StableDiffusionPipeline
from typing import List, Dict, Any, Tuple
import numpy as np
import sys

torch.set_grad_enabled(False)

sys.path.insert(0, './')
assert os.environ["USE_TRT"]


class StableDiffusionPipeline():
    def __init__(self, config_path, **kwargs):
        super().__init__()
        fp = open(config_path, "r")
        config = json.load(fp)
        self.stream_module_list = []
        print(config.keys())
        torch.set_grad_enabled(False)

        self.modules = {'ClipModule': ClipModule,
                        'UNetModule': UNetModule,
                        'VaeModule': VaeModule,
                        'SafetyModule': SafetyModule}

        for key, value in config.items():
            self.modules[key] = self.modules[key](**value)
            # self.modules[key].deploy()
        self.stream_module_list = [self.modules['ClipModule'], self.modules['UNetModule'],
                                   self.modules['VaeModule'], self.modules['SafetyModule']]
        self.default_deploy()

    def default_deploy(self, **kwargs):
        for module in self.stream_module_list:
            module.deploy()


# åˆå§‹åŒ–ç®¡é“
sd_config_file = "stable_diffusion_v1_5/config.json"
sd_pipeline = StableDiffusionPipeline(config_path=sd_config_file)

sd_modules = sd_pipeline.stream_module_list


def warmup_modules(
    modules: List,
    request_template: Dict[str, Any],
    warmup_iters: int = 5,
    progress_bar: bool = True
):
    """é¢„çƒ­æ¨¡å—ä»¥ç¨³å®šæ€§èƒ½"""
    if progress_bar:
        print("\nğŸ”¥ Warming up modules...")
        pbar = tqdm(total=warmup_iters * len([1, 4, 8]) * len(modules),
                    desc="Warmup Progress", unit="op")

    for _ in range(warmup_iters):
        for batch_size in [1, 4, 8, 1]:  # ä½¿ç”¨ä¸åŒæ‰¹æ¬¡å¤§å°è¿›è¡Œé¢„çƒ­
            requests = [deepcopy(request_template) for _ in range(batch_size)]
            for module in modules:
                module.compute(requests)
                for req in requests:
                    for k, v in req.items():
                        if type(v) == torch.Tensor:
                            assert v.is_cpu == False
                torch.cuda.synchronize()
                if progress_bar:
                    pbar.update(1)

    if progress_bar:
        pbar.close()
        print("âœ… Warmup completed")


def save_partial_results(results: Dict, output_path: str):
    """ä¿å­˜éƒ¨åˆ†ç»“æœåˆ°æ–‡ä»¶"""
    with open(output_path, "w") as f:
        json.dump(results, f, ensure_ascii=True)


def profile_module_performance(
    modules: List,
    batch_sizes: List[int],
    request_template: Dict[str, Any],
    num_trials: int = 5,
    progress_bar: bool = True,
    output_path: str = None
) -> Dict[str, Dict[str, float]]:
    """æ€§èƒ½æµ‹è¯•å‡½æ•°ï¼Œå¤šæ¬¡æµ‹é‡æ±‚å¹³å‡ï¼Œå¹¶å¢é‡ä¿å­˜ç»“æœ"""
    latency_profiles = {type(m).__name__: {} for m in modules}

    # åˆ›å»ºè¿›åº¦æ¡
    if progress_bar:
        total_ops = len(batch_sizes) * num_trials * len(modules)
        pbar = tqdm(total=total_ops, desc="Profiling Progress",
                    unit="batch-module")

    import sys
    sys.path.insert(
        0, './25Eurosys-NeuStream-AE/Diffusion/StableDiffusion/H100_SD_FP16_img512/')
    from test_set import prompt_list
    index = 0

    for batch_size in batch_sizes:
        # å­˜å‚¨æ¯æ¬¡è¯•éªŒçš„ç»“æœ
        trial_results = {type(m).__name__: [] for m in modules}

        for trial in range(num_trials):
            # åˆ›å»ºè¯·æ±‚æ‰¹æ¬¡ï¼ˆæ·±æ‹·è´é¿å…å¼•ç”¨é—®é¢˜ï¼‰
            requests = []
            for i in range(batch_size):
                req = deepcopy(request_template)
                req['prompt'] = prompt_list[index % 100]
                index += 1
                requests.append(req)

            for module in modules:
                # ç¡®ä¿CUDAæ“ä½œåŒæ­¥
                if type(module).__name__ == "UNetModule":
                    unet_loop_num = request_template["loop_num"].get(
                        "UNetModule", 50)
                else:
                    unet_loop_num = 1
                torch.cuda.current_stream().synchronize()

                # ç²¾ç¡®æµ‹é‡æ‰§è¡Œæ—¶é—´
                start_time = time.perf_counter()
                for _ in range(unet_loop_num):
                    module.compute(requests)
                torch.cuda.current_stream().synchronize()

                # è®¡ç®—è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
                elapsed = (time.perf_counter() - start_time)

                # å¦‚æœæ˜¯UNetæ¨¡å—ï¼Œé™¤ä»¥å¾ªç¯æ¬¡æ•°
                elapsed /= unet_loop_num

                trial_results[type(module).__name__].append(elapsed)

                # æ›´æ–°è¿›åº¦æ¡
                if progress_bar:
                    pbar.update(1)

        # è®¡ç®—å¹³å‡å»¶è¿Ÿ
        for module_name, latencies in trial_results.items():
            if len(latencies) < 10:
                latencies += latencies
            avg_latency = np.mean(np.sort(latencies)[int(
                len(latencies)*0.1): -int(len(latencies)*0.1)])
            latency_profiles[module_name][str(batch_size)] = float(
                f"{avg_latency:.8f}")

        # å¢é‡ä¿å­˜å½“å‰ç»“æœ
        if output_path:
            new_latency_profiles = {}
            for module_name, v in latency_profiles.items():
                new_latency_profiles[module_name.replace(
                    'Module', '').lower()] = v
            save_partial_results(new_latency_profiles, output_path)

    new_latency_profiles = {}
    for module_name, v in latency_profiles.items():
        new_latency_profiles[module_name.replace('Module', '').lower()] = v

    if progress_bar:
        pbar.close()

    return new_latency_profiles


def generate_filename(image_size: int, config: Dict, max_batch: int) -> str:
    """ç”ŸæˆåŒ…å«é…ç½®ä¿¡æ¯çš„æ–‡ä»¶å"""
    import json
    import toml
    # with open('data/test_config.toml', 'r') as f:
    #     data = toml.load(f)
    # EXP_ID = os.getenv('EXP_ID')
    # json_p = data[EXP_ID]['latency_profile']

    return 'profiles/latency_profile.json'


def main(
    image_size: int = 256,
    min_batch: int = 1,
    max_batch: int = 48,
    num_trials: int = 5,
    warmup_iters: int = 3,
    config_path: str = "stable_diffusion_v1_5/config.json",
    output_dir: str = "profiles",
    progress_bar: bool = True
):
    """
    Stable Diffusion æ¨¡å—æ€§èƒ½åˆ†æå·¥å…·
    
    å‚æ•°:
    image_size (int): å›¾åƒå°ºå¯¸ (é»˜è®¤: 256)
    min_batch (int): æœ€å°æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1)
    max_batch (int): æœ€å¤§æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 40)
    num_trials (int): æ¯ä¸ªæ‰¹å¤„ç†å¤§å°çš„æµ‹è¯•æ¬¡æ•° (é»˜è®¤: 5)
    warmup_iters (int): é¢„çƒ­è¿­ä»£æ¬¡æ•° (é»˜è®¤: 3)
    config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: "stable_diffusion_v1_5/config.json")
    output_dir (str): è¾“å‡ºç›®å½• (é»˜è®¤: "profiles")
    progress_bar (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ (é»˜è®¤: True)
    """
    print(f"\nğŸš€ Starting performance profiling with configuration:")
    print(f"  Image Size: {image_size}x{image_size}")
    print(f"  Batch Sizes: {min_batch} to {max_batch}")
    print(f"  Trials per Batch: {num_trials}")
    print(f"  Warmup Iterations: {warmup_iters}")
    print(f"  Config Path: {config_path}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # é…ç½®å’Œåˆå§‹åŒ–
    # pipeline = initialize_pipeline(config_path)
    modules = sd_modules

    prompt = "a boy studying in Chinese University"
    # è¯·æ±‚æ¨¡æ¿
    REQUEST_TEMPLATE = {
        "prompt": prompt,
        "height": image_size,
        "width": image_size,
        "loop_num": {"UNetModule": 30},  # UNetå¾ªç¯50æ¬¡
        "guidance_scale": 7.5,
        "seed": 81,
        "SLO": 10000,
        "loop_index": {"UNetModule": 0},
        "id": 1,
        "request_time": time.time()
    }

    # æ€§èƒ½æµ‹è¯•å‚æ•°
    BATCH_SIZES = list(range(min_batch, max_batch + 1))

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    filename = generate_filename(image_size, REQUEST_TEMPLATE, max_batch)
    output_path = filename  # os.path.join(output_dir, filename)

    # é¢„çƒ­æ¨¡å—
    warmup_modules(modules, REQUEST_TEMPLATE, warmup_iters, progress_bar)

    # æ‰§è¡Œæ€§èƒ½æµ‹è¯•ï¼Œå¹¶å¢é‡ä¿å­˜ç»“æœ
    if progress_bar:
        print("\nğŸ“Š Starting performance profiling...")
    latency_results = profile_module_performance(
        modules=modules,
        batch_sizes=BATCH_SIZES,
        request_template=REQUEST_TEMPLATE,
        num_trials=num_trials,
        progress_bar=progress_bar,
        output_path=output_path
    )

    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\nâœ… Profiling completed! Results saved to: {output_path}")
    print("\nğŸ“‹ Summary of average latencies (s):")
    for module, data in latency_results.items():
        min_batch_latency = data.get(str(min_batch), "N/A")
        max_batch_latency = data.get(str(max_batch), "N/A")
        print(f"  {module}:")
        print(f"    Batch {min_batch}: {min_batch_latency}")
        print(f"    Batch {max_batch}: {max_batch_latency}")

    return output_path


if __name__ == "__main__":
    fire.Fire(main)
