"Interpreter::backend" = "SyncRing"
'precision' = 'fp16'
scheduler = "Batching"
# [preprocess]
# 'backend' = 'S[DecodeMat,CvtColorMat,ResizeMat,Mat2Tensor,SyncTensor]'
# 'color' = 'rgb'
# instance_num = 2
# next = 'siglip'
# 'resize_h' = '384'
# 'resize_w' = '384'

[batchful]
# PluginCacher is used to cache parameters and input data for (tensorrt) plugin
'backend' = 'S[PluginCacher[TensorrtTensor],SyncTensor]'

cal_request_size_method = "CalTorchBatchSize" # calc. request_size according to torch tensor size

max = '2047'
'model' = '/workspace/VILA/onnx/decode_batchful.onnx'
'model::cache' = './batchful.trt'
next = 'sample'

# [batchless_prefill]
[batchless_prefill]
# request_id node_name 
backend = "S[Jump[kvcache],TensorrtTensor,Jump[kvcache],SyncTensor]" # AppendPositionIDsTensor,
instance_num = 4 
max = '1x2047x2560,1x2047x2560,1x2047x2560,1x2047' 
min = '1x1x2560,1x1x2560,1x1x2560,1x1' 
'model' = '/workspace/VILA/onnx/batchless_prefill.onnx' 
'model::cache' = './batchless_prefill.trt' 

[kvcache]
backend = "S[KVCacheTensor,SyncTensor]" 
max_seq_len = 2048 # out_put : reach_max_seq_len
num_layers = 1 # for debug. 32 for llama.  Auto set?
# [siglip]
# 'backend' = 'SyncTensor[TensorrtTensor]'
# 'mean' = '127.5,127.5,127.5'
# 'model' = '../onnx/visual_encoder.onnx'
# 'model::cache' = '../onnx/visual_encoder.trt'
# next = 'multimodal'
# 'std' = '127.5,127.5,127.5'

# [multimodal]
# backend = 'SyncTensor[MultiModalEmbedsTensor]'
# input_embeds = '../cur_input_embeds_no_im.pt'
# next = 'llm'

# [llm]
# 'backend' = 'SyncTensor[TensorrtTensor]'
# batch_process = 'CpuTensor'
# 'model' = '../onnx/llm.onnx'
# 'model::cache' = '../onnx/llm.trt'

[batchless_decode]
# backend = "RuntimeError"
backend = "S[Jump[kvcache],TensorrtTensor,Jump[kvcache],SyncTensor]" # AppendPositionIDsTensor,
instance_num = 4 
max = '1x1x2560,1x1x2560,1x1x2560,1x1,1x20x2047x128,1x20x2047x128' 
min = '1x1x2560,1x1x2560,1x1x2560,1x1,1x20x1x128,1x20x1x128' 
'model' = '/workspace/VILA/onnx/batchless.onnx' 
'model::cache' = './batchless_decode.trt' 

#TorchPlugin

[sample]
backend = 'S[SampleTensor, Streaming, SyncTensor]'
next = 'check_eos'
temperature = 0.2

[check_eos]
backend = 'S[SyncTensor[IsEosTensorFilter],(if)S[ADD[remove_request_id:],(run)Jump[kvcache]], (else)Add[restart:embed_tokens]]'
# map = "result:other,result:data,request_id:request_id"
# next = "embed_tokens"
eos = 1

# IsEosTensorFilter 写入filter, no result
# filter check 'filter', remove it

[embed_tokens]
backend = "S[SyncTensor[EmbedTokensTensor],ADD[restart:batchful,trt_plugin:batchless_decode]]" # ,Jump[batchful]] 
# restart = "batchful" 
# next = "batchful"

tensor = "/workspace/VILA/embed_tokens.pt"
